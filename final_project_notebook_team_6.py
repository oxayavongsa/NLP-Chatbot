# -*- coding: utf-8 -*-
"""Final Project Notebook - Team 6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tYJ4RQipQBr_AGTIJ4xCnfrARLDrm9cG

The goal of this project is to build a generative chatbot using the **Cornell MovieDialogs Corpus**<br> to carry out multi-turn, context-aware conversations. By leveraging the **T5 architecture**, the <br>chatbot generates coherent, movie-like responses from a dataset containing 220,579 exchanges <br>between 10,292 characters from 617 films (Danescu-Niculescu-Mizil & Lee, 2011).

![cover.webp](data:image/webp;base64,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)
<br>(Volzna, 2024)

### Install Libraries, Import Libraries, Collect Data
"""

# !pip install kaggle
# !pip install transformers torch datasets

# Standard Library Imports
import os
import re
import warnings
from collections import Counter

# Third-Party Imports
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# NLTK Imports
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# PyTorch Imports
import torch
from torch.utils.data import DataLoader, Dataset

# Hugging Face Transformers Imports
from transformers import T5Tokenizer, T5ForConditionalGeneration

# Suppress all warnings
warnings.filterwarnings('ignore')

# Download directly from Kaggle - Thai is using this 'code block' for file path
# Set Kaggle API credentials
os.environ['KAGGLE_USERNAME'] = 'outhaixayavongsa'  # Replace with your Kaggle username
os.environ['KAGGLE_KEY'] = '013bebdbf0776ed704f846ef0b3b3381'  # Replace with your Kaggle API key

# Download the dataset
!kaggle datasets download -d rajathmc/cornell-moviedialog-corpus

# Unzip the dataset (A for All and Press Enter))
!unzip cornell-moviedialog-corpus.zip

# List files to ensure they were extracted from kaggle
extracted_files = os.listdir()
"Extracted files:", extracted_files

"""### Load and Explore Data"""

# Individual Team member's file path

# Define the folder path
# folder_path = 'C:/MS_AAI/CornellMovie/' #Anand data path file
# folder_path = 'C:/Users/Saad/Desktop/Saad Learnings/Python/School Python/Natural Language Processing/Project/CornellMovie/' #Saad data path file
folder_path = './' # Thai data path file

# Initialize a dictionary to store file content
data = {}

# Loop through each file in the directory
for file_name in os.listdir(folder_path):
    if file_name.endswith('.txt'):
        file_path = os.path.join(folder_path, file_name)
        with open(file_path, 'r', encoding='utf-8', errors='replace') as file:
            content = file.readlines()  # Read each line
        data[file_name] = content

# Convert the dictionary to a DataFrame
df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in data.items()]))

# Load completed
print("Data loaded successfully!")

print("1. Basic Information:")
print(f"Number of rows: {df.shape[0]}")
print(f"Number of columns: {df.shape[1]}")
print("\n2. Data Types:")
print(df.dtypes)
print("\n3. Missing Values:")
print(df.isnull().sum())
print("\n4. Descriptive Statistics:")
display(df.describe().T)
print("\n5. Sample Data (first 5 rows):")
display(df.head())

"""### Data Clean and Exploratory Data Analysis"""

# Focusing on Loading movie_lines.txt and movie_conversations.txt where we will parse the files
# lines_file = 'C:/Users/Saad/Desktop/Saad Learnings/Python/School Python/Natural Language Processing/Project/CornellMovie/movie_lines.txt'  #Saad data path file
# conversations_file = 'C:/Users/Saad/Desktop/Saad Learnings/Python/School Python/Natural Language Processing/Project/CornellMovie/movie_conversations.txt' #Saad data path file

# Thai used this file path for Kaggle download
lines_file = 'movie_lines.txt'
conversations_file = 'movie_conversations.txt'

# Function to parse movie_lines.txt
def parse_lines(lines_file):
    lines = {}
    character_names = {}
    with open(lines_file, 'r', encoding='utf-8', errors='replace') as f:
        for line in f:
            parts = line.split(" +++$+++ ")
            if len(parts) == 5:
                line_id = parts[0]
                character_name = parts[3]  # Extract character name
                text = parts[4].strip()
                lines[line_id] = text
                character_names[line_id] = character_name  # Store character names
    return lines, character_names


# Call the function and store the results
lines, character_names = parse_lines(lines_file)

# Construct the DataFrame
lines_df = pd.DataFrame({
    'LineID': list(lines.keys()),
    'Text': list(lines.values()),
    'CharacterName': [character_names[line_id] for line_id in lines.keys()]  # Add CharacterName
})

lines_df.info()  # Check for missing values and data types
lines_df.head()  # Check if the data looks correct

# Function to parse movie_conversations.txt
def parse_conversations(conversations_file):
    conversations = []
    with open(conversations_file, 'r', encoding='utf-8', errors='replace') as f:
        for line in f:
            parts = line.split(" +++$+++ ")
            if len(parts) == 4:
                line_ids = eval(parts[3])  # This is a list of line IDs in a conversation
                conversations.append(line_ids)
    return conversations

# Call the function and store the result in the conversations variable
conversations = parse_conversations(conversations_file)

# Now you can print the conversations variable
print(conversations)

# Function to create dialog pairs
def create_dialog_pairs(conversations, lines):
    """Create dialog pairs from conversations and line mappings."""
    dialog_pairs = []
    for conv in conversations:
        for i in range(len(conv) - 1):
            input_line = lines.get(conv[i], "")
            response_line = lines.get(conv[i + 1], "")
            if input_line and response_line:
                dialog_pairs.append((input_line, response_line))
    return dialog_pairs


# Convert lines to a dictionary and create dialog pairs
dialog_pairs = create_dialog_pairs(
    conversations, lines_df.set_index('LineID')['Text'].to_dict()
)

# Print some dialog pairs for review
for pair in dialog_pairs[:5]:
    print(f"Input: {pair[0]}\nResponse: {pair[1]}\n")

# Convert dialog pairs to DataFrame for easier manipulation
cleaned_dialog_df = pd.DataFrame(dialog_pairs, columns=['input', 'response'])

# Check the first few rows of the DataFrame
cleaned_dialog_df.head()

# Displaying Line length distribution
cleaned_dialog_df['input_length'] = cleaned_dialog_df['input'].apply(lambda x: len(x.split()))
cleaned_dialog_df['response_length'] = cleaned_dialog_df['response'].apply(lambda x: len(x.split()))

plt.figure(figsize=(8, 4))  # Adjusted size for better PDF print compatibility
plt.hist(cleaned_dialog_df['input_length'], bins=30, alpha=0.7, label='Input Line Length')
plt.hist(cleaned_dialog_df['response_length'], bins=30, alpha=0.7, label='Response Line Length')
plt.title('Distribution of Line Lengths (Input vs. Response)')
plt.xlabel('Line Length (Number of Words)')
plt.xlim(0, 150)
plt.ylabel('Frequency')
plt.legend()
plt.tight_layout()  # Ensure everything fits within the figure bounds
plt.show()

# Visualization of the most common words used
all_words = ' '.join(cleaned_dialog_df['input'].tolist() + cleaned_dialog_df['response'].tolist()).split()
word_counts = Counter(all_words)  # Most common words in the cleaned dialog pairs

# Top 20 most common words
common_words = word_counts.most_common(20)
words, counts = zip(*common_words)

# Plot the most common words
plt.figure(figsize=(8, 4))  # Adjust to smaller size if necessary for better printing
plt.bar(words, counts, color='skyblue')  # Use a more visible color for clarity
plt.title('Top 20 Most Common Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')  # Rotate and align text for readability
plt.tight_layout()  # Ensures everything fits within the figure
plt.show()

"""### Text Preprocessing"""

# Download required resources from nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize stopwords and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Preprocessing function
def preprocess_text(text):
    """Lowercase, remove punctuation, tokenize, remove stopwords, and lemmatize text."""

    # 1. Lowercasing
    text = text.lower()

    # 2. Removing Punctuation and Special Characters
    text = re.sub(r'[^\w\s]', '', text)  # Removes punctuation

    # 3. Tokenization
    tokens = word_tokenize(text)

    # 4. Removing Stopwords
    tokens = [word for word in tokens if word not in stop_words]

    # 5. Lemmatization (Optional but recommended)
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    # Join tokens back into a single string
    return ' '.join(tokens)

# Applying the preprocessing to both 'input' and 'response' columns
cleaned_dialog_df['cleaned_input'] = cleaned_dialog_df['input'].apply(preprocess_text)
cleaned_dialog_df['cleaned_response'] = cleaned_dialog_df['response'].apply(preprocess_text)

# Display the first few rows to check the preprocessing
print(cleaned_dialog_df[['cleaned_input', 'cleaned_response']].head())

"""### Additional Preprocessing Step - Handling Rare Words"""

# Step 1: Combine all text (input and response) into a single list of words
all_words = ' '.join(cleaned_dialog_df['input'].tolist() + cleaned_dialog_df['response'].tolist()).split()

# Step 2: Count the frequency of each word
word_counts = Counter(all_words)

# Step 3: Set a threshold (e.g., words that appear fewer than 5 times are considered rare)
threshold = 5
rare_words = {word for word, count in word_counts.items() if count < threshold}

# Step 4: Define a function to replace rare words with '<UNK>'
def replace_rare_words(text, rare_words_set):
    """Replace words that are below the threshold frequency with <UNK>."""
    return ' '.join([word if word not in rare_words_set else '<UNK>' for word in text.split()])

# Step 5: Apply the function to both the input and response columns
cleaned_dialog_df['input'] = cleaned_dialog_df['input'].apply(lambda x: replace_rare_words(x, rare_words))
cleaned_dialog_df['response'] = cleaned_dialog_df['response'].apply(lambda x: replace_rare_words(x, rare_words))

# Step 6: Check a few examples
print(cleaned_dialog_df[['input', 'response']].head())

"""### Data Exploration and Visualization after Preprocessing"""

# 1. Distribution of Input and Response Lengths

# Calculate the length of each cleaned input and response in terms of number of words
cleaned_dialog_df['input_length'] = cleaned_dialog_df['cleaned_input'].apply(lambda x: len(x.split()))
cleaned_dialog_df['response_length'] = cleaned_dialog_df['cleaned_response'].apply(lambda x: len(x.split()))

# Plot histograms for input and response lengths
plt.figure(figsize=(8, 4))
plt.hist(cleaned_dialog_df['input_length'], bins=30, alpha=0.7, label='Input Line Length', color='skyblue')
plt.hist(cleaned_dialog_df['response_length'], bins=30, alpha=0.7, label='Response Line Length', color='salmon')
plt.title('Distribution of Line Lengths (Input vs. Response)')
plt.xlabel('Line Length (Number of Words)')
plt.xlim(0, 50)
plt.ylabel('Frequency')
plt.legend()
plt.tight_layout()  # Ensures everything fits nicely in the figure
plt.show()

# 2. Most Common Words in Cleaned Input and Responses

# Combine all words from both input and response
all_words = ' '.join(cleaned_dialog_df['cleaned_input'].tolist() + cleaned_dialog_df['cleaned_response'].tolist()).split()

# Count the frequency of each word
word_counts = Counter(all_words)

# Get the 20 most common words
common_words = word_counts.most_common(20)
words, counts = zip(*common_words)

# Create a bar plot for the 20 most common words
plt.figure(figsize=(8, 4))
sns.barplot(x=list(words), y=list(counts), palette='viridis')
plt.title('Top 20 Most Common Words After Preprocessing')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')  # Align the labels for better readability
plt.tight_layout()  # Ensure that the layout fits within the figure boundaries
plt.show()

# 3. Word Cloud of Most Common Words

# Generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(all_words))

# Display the word cloud
plt.figure(figsize=(10, 4))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Hide axes
plt.title("Word Cloud of Movie Dialogues After Preprocessing")
plt.tight_layout()  # Ensure the layout fits within the figure boundaries
plt.show()

# 4. Statistics: Average Input and Response Length

# Calculate average input and response length
avg_input_length = cleaned_dialog_df['input_length'].mean()
avg_response_length = cleaned_dialog_df['response_length'].mean()

print(f"Average Input Length (in words): {avg_input_length:.2f}")
print(f"Average Response Length (in words): {avg_response_length:.2f}")

# Optional: Visualizing Input vs Response Length
plt.figure(figsize=(8, 4))
sns.scatterplot(x='input_length', y='response_length', data=cleaned_dialog_df, alpha=0.6)
plt.title('Input vs. Response Length (in words)')
plt.xlabel('Input Length (words)')
plt.ylabel('Response Length (words)')
plt.tight_layout()  # Ensure the layout fits within the figure
plt.show()

# Assuming 'lines_df' is the DataFrame with the 'CharacterName' column
# Count the number of lines spoken by each character
top_characters = lines_df['CharacterName'].value_counts().head(10)

# Plot the top 10 characters by the number of lines spoken
plt.figure(figsize=(8, 4))
sns.barplot(x=top_characters.index, y=top_characters.values, palette='coolwarm')
plt.title("Top 10 Characters by Number of Dialogues", fontsize=16)
plt.xlabel("Character Name", fontsize=12)
plt.ylabel("Number of Dialogues", fontsize=12)
plt.xticks(rotation=45, ha='right')  # Rotate labels and align them for better readability
plt.tight_layout()  # Ensure layout fits within the figure
plt.show()

"""The visuals **after preprocessing** provide insights into the cleaned dataset. The first histogram shows the <br>distribution of **line lengths** for both input and response dialogues, indicating that most conversations are <br>short, with the majority being under 10 words. The bar chart of the **top 20 most common words** reveals <br>frequent usage of conversational terms like "don't," "know," and "I'm," illustrating common speech patterns <br>in movie dialogues. The **word cloud** highlights key dialogue words after preprocessing, showcasing <br>prominent terms such as "know," "one," and "well." The **scatter plot**, comparing input and response lengths,<br> indicates a positive correlation where longer inputs tend to produce longer responses. Lastly, the bar chart<br> displaying the **top 10 characters** by the number of dialogues reveals "Jack" as the most frequent speaker, <br>followed by "Joe" and "George." Together, these visuals demonstrate how the dataset has been effectively <br>cleaned and analyzed for key conversational patterns.

### Check for Imbalance in the Data
"""

# Check for imbalance in character dialogue count
character_dialogue_counts = lines_df['CharacterName'].value_counts()

# Visualize the distribution
plt.figure(figsize=(8, 4))
sns.histplot(character_dialogue_counts, kde=False, bins=50, color='blue')
plt.title('Distribution of Dialogue Count per Character', fontsize=14)
plt.xlabel('Number of Dialogues', fontsize=12)
plt.ylabel('Frequency of Characters', fontsize=12)
plt.tight_layout()  # Ensure layout fits within the figure
plt.show()

# Identify any imbalances (characters with significantly more dialogues)
print(character_dialogue_counts.describe())

"""The **bar chart above** shows the distribution of dialogue counts per character in the dataset, with most characters <br>contributing fewer than 500 lines and a significant majority speaking under 100 lines. Addressing this imbalance<br> could enhance response diversity by including less frequent characters, but it's not essential unless broader <br>character representation is desired.

### Split the Data
"""

# Split the data into training, validation, and test sets
train_data, temp_data = train_test_split(cleaned_dialog_df, test_size=0.2, random_state=42)  # 80% training
val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)  # 10% validation, 10% test

# Display the sizes of each set
print(f"Training set size: {len(train_data)}")
print(f"Validation set size: {len(val_data)}")
print(f"Test set size: {len(test_data)}")

"""## Implement T5 Model
### Setup Tokenizer
"""

# Load T5 tokenizer and model
tokenizer = T5Tokenizer.from_pretrained('t5-small')  # You can also use 't5-base' or 't5-large'
model = T5ForConditionalGeneration.from_pretrained('t5-small')

# Move model to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Check if CUDA (GPU) is available
torch.cuda.is_available()

"""### Modify Dataset Class for T5"""

class DialogDataset(Dataset):
    def __init__(self, data, tokenizer, max_length=512):
        """Initialize the dataset with data, tokenizer, and max_length."""
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        """Return the total number of samples."""
        return len(self.data)

    def __getitem__(self, idx):
        """Retrieve the tokenized input and response for the given index."""
        input_text = self.data.iloc[idx]['cleaned_input']
        response_text = self.data.iloc[idx]['cleaned_response']

        # Prepare the text-to-text task in T5 format
        input_text = f"dialogue: {input_text} </s>"
        response_text = f"{response_text} </s>"

        # Tokenize inputs and responses
        input_ids = self.tokenizer.encode(
            input_text,
            return_tensors='pt',
            max_length=self.max_length,
            padding='max_length',
            truncation=True
        )
        target_ids = self.tokenizer.encode(
            response_text,
            return_tensors='pt',
            max_length=self.max_length,
            padding='max_length',
            truncation=True
        )

        return input_ids.squeeze(), target_ids.squeeze()

class ConversationDataset(Dataset):
    def __init__(self, dataframe, tokenizer, source_len, target_len):
        """Initialize the dataset with dataframe, tokenizer, source and target lengths."""
        self.tokenizer = tokenizer
        self.data = dataframe
        self.source_len = source_len
        self.target_len = target_len
        self.input_text = self.data.input
        self.target_text = self.data.response

    def __len__(self):
        """Return the total number of samples."""
        return len(self.input_text)

    def __getitem__(self, index):
        """Retrieve and encode the input and target text at the given index."""
        # Encode inputs and outputs using the T5 tokenizer
        source_text = str(self.input_text[index])
        target_text = str(self.target_text[index])

        # Tokenize input text
        source = self.tokenizer.batch_encode_plus(
            [source_text],
            max_length=self.source_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        # Tokenize target text
        target = self.tokenizer.batch_encode_plus(
            [target_text],
            max_length=self.target_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        source_ids = source["input_ids"].squeeze()
        source_mask = source["attention_mask"].squeeze()
        target_ids = target["input_ids"].squeeze()

        return {
            "input_ids": source_ids,
            "attention_mask": source_mask,
            "labels": target_ids
        }

"""### Create DataLoader for Training and Validation"""

# Split dataset into training and validation sets
train_size = int(0.8 * len(cleaned_dialog_df))
val_size = len(cleaned_dialog_df) - train_size

# Create the training and validation datasets
train_dataset = DialogDataset(train_data, tokenizer=tokenizer, max_length=50)
val_dataset = DialogDataset(val_data, tokenizer=tokenizer, max_length=50)

# Create DataLoaders for the training and validation sets
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)

"""### Model Training"""

# Define optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

# Training loop
num_epochs = 20

for epoch in range(num_epochs):
    model.train()  # Set model to training mode
    total_loss = 0

    # Loop through the training data
    for batch in tqdm(train_loader, desc=f"Training Epoch {epoch + 1}/{num_epochs}"):
        input_ids = batch[0].to(device)
        labels = batch[1].to(device)

        # Generate attention mask based on input_ids
        attention_mask = (input_ids != tokenizer.pad_token_id).long().to(device)

        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_train_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch + 1}/{num_epochs}, Average Training Loss: {avg_train_loss:.4f}")

# Save the trained model and tokenizer
model_dir = 'models/t5_chatbot_final'

# Save model and tokenizer to the specified directory
model.save_pretrained(model_dir)
tokenizer.save_pretrained(model_dir)

"""### Model Evaluation"""

# Function to evaluate model and print metrics
def evaluate_model(model, dataloader, device):
    """Evaluate the model on the given dataloader and return loss and predictions."""
    model.eval()  # Set model to evaluation mode
    total_loss = 0
    all_predictions = []
    all_targets = []

    with torch.no_grad():  # Disable gradient calculation for evaluation
        for batch in tqdm(dataloader, desc="Evaluating"):
            input_ids, target_ids = batch
            input_ids = input_ids.to(device)
            target_ids = target_ids.to(device)

            # Forward pass
            outputs = model(input_ids=input_ids, labels=target_ids)
            val_loss = outputs.loss

            total_loss += val_loss.item()

            # Collect predictions and targets for metrics calculation
            predicted_ids = torch.argmax(outputs.logits, dim=-1).cpu().numpy().flatten()
            all_predictions.extend(predicted_ids)
            all_targets.extend(target_ids.cpu().numpy().flatten())

    avg_loss = total_loss / len(dataloader)
    return avg_loss, all_predictions, all_targets


# Evaluate model on validation data after training
val_loss, val_predictions, val_targets = evaluate_model(model, val_loader, device)
print(f"Validation Loss: {val_loss:.4f}")

"""### Model Metrics"""

# Calculate and print evaluation metrics
accuracy = accuracy_score(val_targets, val_predictions)
precision = precision_score(val_targets, val_predictions, average='weighted')
recall = recall_score(val_targets, val_predictions, average='weighted')
f1 = f1_score(val_targets, val_predictions, average='weighted')

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")

# Visualizing the Metrics
metrics = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-Score': f1}
metrics_names = list(metrics.keys())
metrics_values = list(metrics.values())

plt.figure(figsize=(8, 4))
sns.barplot(x=metrics_names, y=metrics_values, palette='viridis')
plt.title("Evaluation Metrics", fontsize=14)
plt.ylim(0, 1)
plt.ylabel("Score", fontsize=12)
plt.xlabel("Metric", fontsize=12)
plt.xticks(rotation=0)
plt.tight_layout()  # Ensures everything fits within the figure
plt.show()

# Import the necessary libraries
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration

# Load the pre-trained T5 model and tokenizer (you can load your fine-tuned model here)
model_name = 'models/t5_chatbot_final'
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# Move the model to the appropriate device (GPU or CPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

def generate_response(conversation_history):
    """Generate a chatbot response based on the conversation history."""
    # Concatenate conversation history for context
    input_text = " ".join(conversation_history)

    # Tokenize and encode the input text
    input_ids = tokenizer.encode(f"dialogue: {input_text}", return_tensors="pt").to(device)  # Move to correct device

    # Generate the output using the model
    output_ids = model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)

    # Decode the output into a human-readable response
    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    return response

# Interactive chatbot loop to converse with the user
if __name__ == "__main__":
    while True:
        user_input = input("You: ")
        if user_input.lower() == "exit":
            print("Chatbot: Goodbye!")
            break
        chatbot_response = generate_response([user_input])
        print(f"Chatbot: {chatbot_response}")

"""## References<br>
Danescu-Niculescu-Mizil, C., & Lee, L. (2011). *Cornell movie-dialogs corpus.* Cornell <br>University. https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html

Vozna, A. (2024, June 13). *AI chatbot development: A complete guide.<br>* https://gloriumtech.com/ai-chatbot-development-a-complete-guide/
"""